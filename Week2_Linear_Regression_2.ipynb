{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6d6098",
   "metadata": {},
   "source": [
    "\n",
    "# DX799 â€” Week 2 Jupyter Notebook: Linear Regression 2  \n",
    "**Topic:** Lasso, Ridge, Elastic Net on your Capstone dataset  \n",
    "**Author:** <Your Name>  \n",
    "**Date:** <Auto/Today>  \n",
    "\n",
    "> Use this notebook to run regularized linear models and capture results for **Milestone One**. Replace placeholders with your dataset and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4480e9d",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“‹ Milestone One Alignment (Quick Map)\n",
    "- **Breadth (Weeks 1â€“6):** This notebook contributes **Week 2** coverage: lasso, ridge, elastic net.  \n",
    "- **Depth (choose 1â€“2 weeks):** Use **Week 2** as one deep-dive.  \n",
    "- **Overfitting prevention:** Cross-validation, regularization, learning curves, and holdout set.  \n",
    "- **Metrics & tuning:** RMSE/MAE/RÂ²; GridSearchCV across alphas; ElasticNet `l1_ratio`.  \n",
    "- **Expected vs unexpected:** Capture observations in the Summary section.  \n",
    "- **EDA support:** Basic EDA, correlation heatmap, VIF for multicollinearity.  \n",
    "- **External sources:** Cite at least one high-quality source in Yellowdig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ca1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "\n",
    "# Optional diagnostics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45b0b6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load Data  \n",
    "Replace the path below and identify your **target** and **feature** columns.  \n",
    "If you already engineered features in earlier weeks, import that cleaned dataset here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load your dataset ---\n",
    "# Example: df = pd.read_csv('data/your_clean_dataset.csv')\n",
    "df = pd.DataFrame()  # placeholder; replace with your actual load\n",
    "\n",
    "# Quick sanity check\n",
    "display(df.head())\n",
    "display(df.describe(include='all').T.head(20))\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e39dc",
   "metadata": {},
   "source": [
    "\n",
    "### Select Target and Features\n",
    "Set your `TARGET` and pick feature columns. You can also drop leakage columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd13117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configure columns ---\n",
    "TARGET = 'your_target'  # <-- replace\n",
    "feature_cols = [c for c in df.columns if c != TARGET]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb981abd",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Split Data\n",
    "Use a holdout set for honest evaluation after model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023781f2",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Preprocessing  \n",
    "- Numeric: Standardize.  \n",
    "- Categorical: One-hot encode.  \n",
    "- (Optional) Polynomial/interaction terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Identify column types ---\n",
    "num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Optional: add polynomial features for numeric only (comment in if useful)\n",
    "add_poly = False  # set True if you want polynomial terms\n",
    "degree = 2\n",
    "\n",
    "if add_poly:\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), num_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caeb2f0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Baseline: Ordinary Least Squares (no penalty)  \n",
    "Use as a baseline to compare against regularized models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ols = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "ols.fit(X_train, y_train)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "\n",
    "def regression_report(y_true, y_pred, label=\"Model\"):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return pd.Series({'RMSE': rmse, 'MAE': mae, 'R2': r2}, name=label)\n",
    "\n",
    "report_ols = regression_report(y_test, y_pred_ols, 'OLS')\n",
    "report_ols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512126c",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Ridge Regression (L2)  \n",
    "Grid-search over `alpha` with CV. Ridge shrinks many coefficients but rarely to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'model__alpha': np.logspace(-3, 3, 13)  # 0.001 to 1000\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "ridge_gs = GridSearchCV(ridge, ridge_param_grid, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "ridge_gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge_gs.predict(X_test)\n",
    "report_ridge = regression_report(y_test, y_pred_ridge, 'Ridge')\n",
    "pd.DataFrame({\n",
    "    'best_params': [ridge_gs.best_params_],\n",
    "    'cv_score_rmse_neg': [ridge_gs.best_score_]\n",
    "}), report_ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7101fd",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Lasso Regression (L1)  \n",
    "Grid-search over `alpha` with CV. Lasso performs feature selection by driving some coefficients to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55937b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "lasso_param_grid = {\n",
    "    'model__alpha': np.logspace(-3, 1, 9)  # 0.001 to 10\n",
    "}\n",
    "\n",
    "lasso_gs = GridSearchCV(lasso, lasso_param_grid, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "lasso_gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso = lasso_gs.predict(X_test)\n",
    "report_lasso = regression_report(y_test, y_pred_lasso, 'Lasso')\n",
    "pd.DataFrame({\n",
    "    'best_params': [lasso_gs.best_params_],\n",
    "    'cv_score_rmse_neg': [lasso_gs.best_score_]\n",
    "}), report_lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99adaef",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Elastic Net (Î± blend of L1 & L2)  \n",
    "Tune both `alpha` (regularization strength) and `l1_ratio` (mix of L1 vs L2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enet = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', ElasticNet(max_iter=10000))\n",
    "])\n",
    "\n",
    "enet_param_grid = {\n",
    "    'model__alpha': np.logspace(-3, 1, 9),\n",
    "    'model__l1_ratio': np.linspace(0.1, 0.9, 9)\n",
    "}\n",
    "\n",
    "enet_gs = GridSearchCV(enet, enet_param_grid, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "enet_gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred_enet = enet_gs.predict(X_test)\n",
    "report_enet = regression_report(y_test, y_pred_enet, 'ElasticNet')\n",
    "pd.DataFrame({\n",
    "    'best_params': [enet_gs.best_params_],\n",
    "    'cv_score_rmse_neg': [enet_gs.best_score_]\n",
    "}), report_enet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b56660",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Compare Models  \n",
    "Summarize holdout metrics and pick a winner. Discuss trade-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comparison = pd.concat([report_ols, report_ridge, report_lasso, report_enet], axis=1).T.sort_values('RMSE')\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e2091",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Coefficient Inspection  \n",
    "Inspect learned coefficients to interpret model behavior.  \n",
    "> Note: After preprocessing, use the fitted pipeline to extract feature names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f19a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_names(preprocessor):\n",
    "    names = []\n",
    "    if hasattr(preprocessor, 'transformers_'):\n",
    "        for name, trans, cols in preprocessor.transformers_:\n",
    "            if name == 'num':\n",
    "                # PolynomialFeatures may change feature set\n",
    "                if hasattr(trans, 'named_steps') and 'poly' in trans.named_steps:\n",
    "                    poly = trans.named_steps['poly']\n",
    "                    base_names = cols\n",
    "                    names += poly.get_feature_names_out(base_names).tolist()\n",
    "                else:\n",
    "                    names += list(cols)\n",
    "            elif name == 'cat':\n",
    "                ohe = trans.named_steps['onehot']\n",
    "                ohe_names = ohe.get_feature_names_out(cols).tolist()\n",
    "                names += ohe_names\n",
    "    return names\n",
    "\n",
    "# Example: Ridge coefficients\n",
    "best_ridge = ridge_gs.best_estimator_\n",
    "feat_names = get_feature_names(best_ridge.named_steps['prep'])\n",
    "\n",
    "coefs = best_ridge.named_steps['model'].coef_\n",
    "coef_df = pd.DataFrame({'feature': feat_names, 'coef': coefs}).sort_values('coef', key=lambda s: s.abs(), ascending=False)\n",
    "coef_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a008a6b",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Overfitting Diagnostics  \n",
    "- Cross-validation scores distribution.  \n",
    "- Simple size-based learning effect (optional).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_rmse(model, X, y, cv):\n",
    "    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "scores_ridge = cv_rmse(ridge_gs.best_estimator_, X_train, y_train, cv)\n",
    "scores_lasso = cv_rmse(lasso_gs.best_estimator_, X_train, y_train, cv)\n",
    "scores_enet  = cv_rmse(enet_gs.best_estimator_,  X_train, y_train, cv)\n",
    "\n",
    "print(\"CV RMSE (mean Â± std)\")\n",
    "print(f\"Ridge: {scores_ridge.mean():.4f} Â± {scores_ridge.std():.4f}\")\n",
    "print(f\"Lasso: {scores_lasso.mean():.4f} Â± {scores_lasso.std():.4f}\")\n",
    "print(f\"ENet : {scores_enet.mean():.4f} Â± {scores_enet.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598fdfe",
   "metadata": {},
   "source": [
    "\n",
    "### CV Score Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.boxplot([scores_ridge, scores_lasso, scores_enet], labels=['Ridge','Lasso','ElasticNet'])\n",
    "plt.title('CV RMSE Distribution')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4440cbe",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Multicollinearity Snapshot (Numeric Only)  \n",
    "- Correlation heatmap and quick VIF calculation to see redundancy risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute correlation on numeric columns\n",
    "if len(num_cols) > 1:\n",
    "    corr = X_train[num_cols].corr()\n",
    "    display(corr)\n",
    "\n",
    "# Quick VIF (numeric only). Requires no missing values.\n",
    "def compute_vif(df_num):\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    import statsmodels.api as sm\n",
    "    # drop NA and constant-only cols\n",
    "    z = df_num.dropna().copy()\n",
    "    z = z.loc[:, z.std() > 0]\n",
    "    Xc = sm.add_constant(z)\n",
    "    vifs = []\n",
    "    for i in range(1, Xc.shape[1]):  # skip constant\n",
    "        vifs.append(variance_inflation_factor(Xc.values, i))\n",
    "    return pd.DataFrame({'feature': z.columns, 'VIF': vifs})\n",
    "\n",
    "try:\n",
    "    if len(num_cols) > 1:\n",
    "        vif_df = compute_vif(X_train[num_cols])\n",
    "        display(vif_df.sort_values('VIF', ascending=False).head(15))\n",
    "except Exception as e:\n",
    "    print(\"VIF computation skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295248",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Summary & Discussion (Copy to Milestone One)\n",
    "- **Best model** and why (metrics + interpretability).  \n",
    "- **Overfitting controls** used and their effect.  \n",
    "- **Expected vs unexpected** patterns (e.g., selected features, sign of coefficients, stability).  \n",
    "- **Role of EDA** in feature choices and model interpretation.  \n",
    "- **Next steps** for Weeks 3â€“6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a122e3",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Yellowdig Helper (External Source to Share)\n",
    "- Paste a high-quality source link you used to clarify a concept (e.g., scikit-learn docs on Lasso/Ridge/ElasticNet).  \n",
    "- Write 3â€“5 lines on *why* it is high quality (clarity, rigor, examples, reproducibility).  \n",
    "- Reply to 2 peers with one insight each.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}